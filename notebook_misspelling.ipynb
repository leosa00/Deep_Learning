{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoZ_aGijzQee"
      },
      "outputs": [],
      "source": [
        "###USED TO MOUNT GOOGLE DRIVE TO SAVE OUTPUT CSVs IF THE NOTEBOOK IS RUN ON GOOGLE COLAB\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/Deep_Learning_Project\"\n",
        "\n",
        "os.makedirs(DRIVE_PROJECT_PATH, exist_ok=True)\n",
        "print(f\"Project directory ensures at: {DRIVE_PROJECT_PATH}\")\n",
        "# ----------------------------------\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQd9KmUkAtoK"
      },
      "outputs": [],
      "source": [
        "###LOADS TED TALKS DATA SET AND ONLY TAKES THE ENGLISH AND SPANISH COLUMNS\n",
        "\n",
        "import random\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import os\n",
        "\n",
        "MODEL_OUTPUT_DIR = os.path.join(DRIVE_PROJECT_PATH, \"byt5-ted-en-es-checkpoints\")\n",
        "\n",
        "print(\"Loading dataset TankuVie/ted_talks_multilingual_parallel_corpus...\")\n",
        "raw_datasets = load_dataset(\"TankuVie/ted_talks_multilingual_parallel_corpus\")\n",
        "full_train = raw_datasets[\"train\"]\n",
        "\n",
        "def non_empty_example(example):\n",
        "    en = example.get(\"en\", \"\")\n",
        "    es = example.get(\"es\", \"\")\n",
        "    return (\n",
        "        isinstance(en, str)\n",
        "        and isinstance(es, str)\n",
        "        and en.strip() != \"\"\n",
        "        and es.strip() != \"\"\n",
        "    )\n",
        "\n",
        "filtered_train = full_train.filter(non_empty_example)\n",
        "\n",
        "print(\"Total usable examples:\", len(filtered_train))\n",
        "\n",
        "\n",
        "print(\"Calculating maximum character lengths...\")\n",
        "\n",
        "\n",
        "en_lengths = [len(s) for s in filtered_train[\"en\"]]\n",
        "max_en_length = max(en_lengths)\n",
        "\n",
        "es_lengths = [len(s) for s in filtered_train[\"es\"]]\n",
        "max_es_length = max(es_lengths)\n",
        "\n",
        "print(\"--- RESULTS ---\")\n",
        "print(f\"Maximum character length in 'en' column: {max_en_length}\")\n",
        "print(f\"Maximum character length in 'es' column: {max_es_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBZNNDdAC8H-"
      },
      "outputs": [],
      "source": [
        "###LOADS THE COMMON MISSPELLED_WORDS DATASET WHICH IS USED TO ADD MISSPELLINGS\n",
        "\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "misspelling_dir_path = kagglehub.dataset_download(\"fazilbtopal/misspelled-words\")\n",
        "\n",
        "\n",
        "FILE_NAME = \"misspelled.csv\"\n",
        "misspelling_file_path = os.path.join(misspelling_dir_path, FILE_NAME)\n",
        "\n",
        "print(\"Path to dataset directory:\", misspelling_dir_path)\n",
        "print(\"Path to CSV file:\", misspelling_file_path)\n",
        "\n",
        "try:\n",
        "    df_misspelled = pd.read_csv(misspelling_file_path)\n",
        "    print(\"Successfully loaded the CSV file.\")\n",
        "    print(\"Head of DataFrame:\")\n",
        "    print(df_misspelled.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{FILE_NAME}' was not found inside the downloaded directory.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLx9_AlUHOGy",
        "outputId": "98742ea0-424a-4687-ea5a-050b94e364ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully created MISSPELLED_DICT with 7762 entries.\n"
          ]
        }
      ],
      "source": [
        "###CLEANS UP AND CREATES THE MISSPELLED WORD DICTIONARY\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "misspelling_map = (\n",
        "    df_misspelled.groupby('label')['input']\n",
        "    .apply(list)\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "MISSPELLED_DICT = {}\n",
        "for k, v in misspelling_map.items():\n",
        "    if isinstance(k, str):\n",
        "        key = k.lower()\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    values = []\n",
        "    for w in v:\n",
        "        if isinstance(w, str):\n",
        "            values.append(w.lower())\n",
        "\n",
        "    if values:\n",
        "        MISSPELLED_DICT[key] = values\n",
        "\n",
        "print(\"Successfully created MISSPELLED_DICT with\", len(MISSPELLED_DICT), \"entries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MpTWjXiJRRi"
      },
      "outputs": [],
      "source": [
        "#This function generates corrupted text based on the misspelling dict and a corruption rate\n",
        "def generate_corrupted_text(text_to_corrupt, misspelling_dict, corruption_rate):\n",
        "    \"\"\"Generates a single corrupted version of the input text.\"\"\"\n",
        "    parts = re.split(r'(\\W+)', text_to_corrupt)\n",
        "    new_parts = []\n",
        "\n",
        "    for part in parts:\n",
        "        if re.match(r'\\w+', part):\n",
        "            lower_word = part.lower()\n",
        "            misspellings = misspelling_dict.get(lower_word)\n",
        "            # Only corrupt if misspellings exist AND a random number is below the rate\n",
        "            if misspellings and random.random() < corruption_rate:\n",
        "                new_part = random.choice(misspellings)\n",
        "            else:\n",
        "                new_part = part\n",
        "        else:\n",
        "            new_part = part\n",
        "\n",
        "        new_parts.append(new_part)\n",
        "\n",
        "    return \"\".join(new_parts)\n",
        "\n",
        "#Uses the generate_corrupted_text function to create multiple misspelled sentences with three corruption\n",
        "#levels 0.1,0.2 and 0.3. This function also clips the length of all inputs/outputs to be max 256 characters, since\n",
        "#this is the length that the models were fine tuned on\n",
        "def corrupt_and_clip_example_multi_rate(example, misspelling_dict, max_length=256, corruption_rates=[0.1, 0.2, 0.3]):\n",
        "\n",
        "    def clip_text(text, length):\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        return text[:length]\n",
        "    clipped_en = clip_text(example[\"en\"], max_length)\n",
        "    clipped_es = clip_text(example[\"es\"], max_length)\n",
        "    result = {\n",
        "        \"en\": clipped_en,\n",
        "        \"es\": clipped_es,\n",
        "    }\n",
        "\n",
        "    text_to_corrupt = clipped_en\n",
        "    for rate in corruption_rates:\n",
        "        misspelled_sentence = generate_corrupted_text(\n",
        "            text_to_corrupt,\n",
        "            misspelling_dict,\n",
        "            rate\n",
        "        )\n",
        "\n",
        "        misspelled_sentence_clipped = clip_text(misspelled_sentence, max_length)\n",
        "\n",
        "        column_name = f\"corruption_{rate}\".replace('.', '_') # Use 'corruption_0_1' for 0.1\n",
        "        result[column_name] = misspelled_sentence_clipped\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "TARGET_RATES = [0.1, 0.2, 0.3]\n",
        "\n",
        "mutated_dataset_map = filtered_train.map(\n",
        "    lambda x: corrupt_and_clip_example_multi_rate(\n",
        "        x,\n",
        "        MISSPELLED_DICT,\n",
        "        256,\n",
        "        corruption_rates=TARGET_RATES\n",
        "    ),\n",
        "    batched=False,\n",
        ")\n",
        "\n",
        "new_corruption_columns = [f\"corruption_{rate}\".replace('.', '_') for rate in TARGET_RATES]\n",
        "all_target_columns = [\"en\", \"es\"] + new_corruption_columns\n",
        "\n",
        "mutated_dataset = mutated_dataset_map.select_columns(all_target_columns)\n",
        "\n",
        "mutated_dataset_dict = DatasetDict({\n",
        "    \"train\": mutated_dataset\n",
        "})\n",
        "\n",
        "print(\"\\n--- Mutated Dataset Preview ---\")\n",
        "print(mutated_dataset_dict)\n",
        "print(mutated_dataset_dict[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUmA-MiuJKy0"
      },
      "outputs": [],
      "source": [
        "### HERE THE DATASET WITH CORRUPTED ENGLISH SENTENCES IS SAVED AS A CSV\n",
        "dataset_to_save = mutated_dataset_dict[\"train\"]\n",
        "output_path = \"/content/drive/MyDrive/Deep_Learning_Project/corrupted_dataset.csv\"\n",
        "\n",
        "print(f\"Starting to save dataset to: {output_path}\")\n",
        "\n",
        "dataset_to_save.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"\\nDataset successfully saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqWzS5m_6w70"
      },
      "outputs": [],
      "source": [
        "###IMPORTS OUR T5 and BYT5 MODELS\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "MODEL_PATH_BYT5= \"malinhauglandh/byt5-en-es-translation\"\n",
        "MODEL_PATH_T5 = \"InaMartini/t5-en-es-translation\"\n",
        "TASK_PREFIX = \"translate English to Spanish: \"\n",
        "MAX_LENGTH = 256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbZoO_a-5gof"
      },
      "outputs": [],
      "source": [
        "###THIS FUNCTION TAKES THE MODEL AS AN INPUT AND THE TEXT TO TRANSLATE\n",
        "def translate_sentence(model, tokenizer, device, texts_to_translate):\n",
        "    if isinstance(texts_to_translate, str):\n",
        "        texts_to_translate = [texts_to_translate]\n",
        "\n",
        "    input_texts = [TASK_PREFIX + text for text in texts_to_translate]\n",
        "\n",
        "    input_ids = tokenizer(\n",
        "        input_texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=MAX_LENGTH,\n",
        "            num_beams=4,\n",
        "            do_sample=False,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "    translated_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
        "    return translated_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlVde_aJF8zl"
      },
      "outputs": [],
      "source": [
        "###RUNS THE TRANSLATION CODE ON THE ORIGINAL ENGLISH TEXT FOR A SPECIFIED MODEL\n",
        "###SAVES THE OUTPUT TO A CSV\n",
        "def model_runner(model_path, model_name):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading model from: {model_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        results = []\n",
        "        for i in range(0, 1000):\n",
        "            sentenceEn = mutated_dataset_dict[\"train\"][i]['en']\n",
        "            translated_text = translate_sentence(model, tokenizer, device, sentenceEn)\n",
        "            results.append({\"en\": sentenceEn, \"es\": translated_text})\n",
        "            print(f\"{i}/1000\")\n",
        "        df_results = pd.DataFrame(results)\n",
        "        output_csv_path = os.path.join(DRIVE_PROJECT_PATH, f\"{model_name}_translated.csv\")\n",
        "        df_results.to_csv(output_csv_path, index=False)\n",
        "        print(f\"\\n✅ Translations saved to: {output_csv_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFATAL ERROR: Could not load or run the model.\")\n",
        "        print(f\"Please check your MODEL_PATH variable: {model_path}\")\n",
        "        print(f\"Error details: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDNAXHlb7474"
      },
      "outputs": [],
      "source": [
        "###RUNS THE TRANSLATION CODE ON THE MISSPELLED ENLGISH SENTENCES, ON ALL CORRUPTION LEVELS\n",
        "###SAVES THE OUTPUT TO A CSV\n",
        "def model_runner_misspelled(model_path, model_name):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading model from: {model_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        results = []\n",
        "        for i in range(0, 1000):\n",
        "            corrupted_en_0_1 = mutated_dataset_dict[\"train\"][i]['corruption_0_1']\n",
        "            corrupted_en_0_2 = mutated_dataset_dict[\"train\"][i]['corruption_0_2']\n",
        "            corrupted_en_0_3 = mutated_dataset_dict[\"train\"][i]['corruption_0_3']\n",
        "\n",
        "            # Translate corrupted sentences\n",
        "            translated_0_1 = translate_sentence(model, tokenizer, device, corrupted_en_0_1)\n",
        "            translated_0_2 = translate_sentence(model, tokenizer, device, corrupted_en_0_2)\n",
        "            translated_0_3 = translate_sentence(model, tokenizer, device, corrupted_en_0_3)\n",
        "\n",
        "            results.append({\n",
        "                \"en_corruption_1\": corrupted_en_0_1,\n",
        "                \"es_corruption_1\": translated_0_1,\n",
        "                \"en_corruption_2\": corrupted_en_0_2,\n",
        "                \"es_corruption_2\": translated_0_2,\n",
        "                \"en_corruption_3\": corrupted_en_0_3,\n",
        "                \"es_corruption_3\": translated_0_3\n",
        "            })\n",
        "            print(f\"{i}/1000\")\n",
        "\n",
        "        df_results = pd.DataFrame(results)\n",
        "        output_csv_path = os.path.join(DRIVE_PROJECT_PATH, f\"{model_name}_corrupt_translated.csv\")\n",
        "        df_results.to_csv(output_csv_path, index=False)\n",
        "        print(f\"\\n✅ Corrupted translations saved to: {output_csv_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFATAL ERROR: Could not load or run the model.\")\n",
        "        print(f\"Please check your MODEL_PATH variable: {model_path}\")\n",
        "        print(f\"Error details: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkTswVsGMyLP"
      },
      "outputs": [],
      "source": [
        "#RUNS THE MODEL RUNNER_MISSPELLED ON T5\n",
        "model_runner_misspelled(MODEL_PATH_T5, \"t5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5vnovw2moAb"
      },
      "outputs": [],
      "source": [
        "#RUNS THE MODEL RUNNER_MISSPLEED ON BYT5\n",
        "model_runner_misspelled(MODEL_PATH_BYT5, \"byt5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2t9z6rr-VEA"
      },
      "outputs": [],
      "source": [
        "#RUNS THE MODEL RUNNER ON BYT5\n",
        "model_runner(MODEL_PATH_BYT5, \"byt5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg6MwSUU9j1-"
      },
      "outputs": [],
      "source": [
        "#RUNS THE MODEL RUNNER ON T5\n",
        "model_runner(MODEL_PATH_T5,\"t5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
